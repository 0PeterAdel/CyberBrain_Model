# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aEEZhXlf3q_31TNFDy6tHGOSMGqLKDv-

**1.Install Requiremets**
"""

!pip3 install unsloth transformers datasets bitsandbytes accelerate

!pip install --upgrade pip
!pip install torch==2.5.1+cu118 --index-url https://download.pytorch.org/whl/cu118
!pip install torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Check GPU
import torch
print(torch.cuda.is_available())

"""**2.Model Loading with Unsloth:**"""

from unsloth import FastLanguageModel
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/DeepSeek-R1-Distill-Qwen-14B",
    max_seq_length = 2048,
    load_in_4bit = True,
)

"""**3.LoRA Configuration:**"""

model = FastLanguageModel.get_peft_model(
    model,
    r=16,  # Adjust the LoRA rank as needed
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],  # Specify key modules
    lora_alpha=16,
    lora_dropout=0.05,  # You can adjust dropout to prevent overfitting
    bias="none"
)

"""
***Load DataSet***"""

from datasets import load_dataset
dataset = load_dataset("json", data_files="Set.json")

# Define the text template
prompt_template = """### Instruction:
{instruction}

### Response:
{response}"""

# Get the EOS token from the tokenizer
EOS_TOKEN = tokenizer.eos_token

# Data formatting function
def formatting_func(examples):
    texts = []
    for inst, resp in zip(examples["instruction"], examples["Response"]):
        text = prompt_template.format(instruction=inst, response=resp) + EOS_TOKEN
        texts.append(text)
    return {"text": texts}

# Apply formatting to the dataset
dataset = dataset.map(formatting_func, batched=True)

"""***Splitting the dataset***"""

dataset = dataset["train"].train_test_split(test_size=0.1)
train_dataset = dataset["train"]
eval_dataset = dataset["test"]

# Print the first 5 examples from the training dataset
for i in range(5):
    print("Example", i, ":\n", train_dataset[i]["text"], "\n")

"""

---


**4. Configure Training Arguments**"""

from transformers import TrainingArguments

training_args = TrainingArguments(
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    warmup_steps=5,
    max_steps=100,
    learning_rate=2e-4,
    fp16=True,
    logging_steps=10,
    output_dir="outputs",
    optim="adamw_8bit",
    eval_strategy="steps",  # Use eval_strategy instead of evaluation_strategy
    eval_steps=20,
)

from trl import SFTTrainer

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    dataset_text_field="text",
    max_seq_length=2048,
    args=training_args,
)

"""**5. Start Train**"""

trainer.train()

"""---



---

**6. Model Evaluation:**
"""

trainer.evaluate()

"""

---

**Save form and tokenizer:**"""

model.save_pretrained("CyberBrain_model")
tokenizer.save_pretrained("CyberBrain_model")

"""

---



---

***Upload the model to Hugging Face Hub :***"""

!pip install huggingface_hub

"""***Login***"""

!huggingface-cli login

"""***publish***"""

model.push_to_hub("PeterAdel/CyberBrain_R1")
tokenizer.push_to_hub("PeterAdel/CyberBrain_R1")

"""

---



---

**Test Prompt After Train**"""

# Create a test script using the template
prompt = """### Instruction:
ما هي العواقب المحتملة لاستغلال ثغرة CVE-2023-29351؟

### Response:"""

# Convert text to token format and move input to GPU
input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to("cuda")

# Generate answer with specified number of new tokens
outputs_prompt = model.generate(input_ids, max_new_tokens=200)

# Decode the generated tokens and print the answer
print(tokenizer.decode(outputs_prompt[0], skip_special_tokens=True))

# Create a test script using the template
prompt = """### Instruction:
كيف تنفذ لاستغلال ثغرة CVE-2023-29351؟

### Response:"""

# Convert text to token format and move input to GPU
input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to("cuda")

# Generate answer with specified number of new tokens
outputs_prompt = model.generate(input_ids, max_new_tokens=200)

# Decode the generated tokens and print the answer
print(tokenizer.decode(outputs_prompt[0], skip_special_tokens=True))